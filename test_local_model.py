import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def test_model():
    # 1. æŒ‡å‘æˆ‘ä»¬åˆšåˆšä¸‹è½½å¥½çš„ã€æœ¬åœ°è·¯å¾„ã€‘
    # ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„å‡å¯ï¼Œè¿™é‡Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
    local_model_path = "./models/Qwen1.5-0.5B-Chat" 
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(local_model_path):
        print(f"âŒ æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {local_model_path}")
        print("è¯·å…ˆè¿è¡Œ python download_model.py ä¸‹è½½æ¨¡å‹ã€‚")
        # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå›é€€åˆ°äº‘ç«¯ ID (å¯é€‰)
        model_id = "Qwen/Qwen1.5-0.5B-Chat"
        print(f"âš ï¸ å°†å°è¯•ä» Hugging Face åœ¨çº¿åŠ è½½: {model_id}")
    else:
        print(f"âœ… å‘ç°æœ¬åœ°æ¨¡å‹: {local_model_path}")
        model_id = local_model_path

    # 2. è®¾ç½®è®¾å¤‡ (Mac ä¸Šé€šå¸¸æ˜¯ cpuï¼Œå¦‚æœæ˜¯ MèŠ¯ç‰‡å¯ä»¥ç”¨ mpsï¼Œä½†è¿™é‡Œå…ˆç”¨é€šç”¨é€»è¾‘)
    # æ³¨æ„ï¼šmps (Metal Performance Shaders) æ˜¯ Mac çš„ GPU åŠ é€Ÿï¼Œä½† pytorch æ”¯æŒæƒ…å†µè§†ç‰ˆæœ¬è€Œå®š
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps" 
    else:
        device = "cpu"
        
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    try:
        # 3. åŠ è½½åˆ†è¯å™¨ (ä»æœ¬åœ°)
        print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        # 4. åŠ è½½æ¨¡å‹ (ä»æœ¬åœ°) å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(model_id).to(device)

        print("ğŸ‰ æ¨¡å‹åŠ è½½å®Œæˆï¼å‡†å¤‡ç”Ÿæˆæµ‹è¯•...")

        # 5. ä½¿ç”¨å¯¹è¯æ¨¡æ¿è¿›è¡Œæµ‹è¯•
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©ç†ã€‚"},
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·å†™ä¸€é¦–ç°ä»£è¯—ã€‚"}
        ]
        
        # ä½¿ç”¨ apply_chat_template è‡ªåŠ¨æ ¼å¼åŒ–è¾“å…¥
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        print(f"\næ„å»ºçš„ Prompt:\n{text}")
        
        model_inputs = tokenizer([text], return_tensors="pt").to(device)
        
        # ç”Ÿæˆå›å¤
        # ---------------------------------------------------------
        # é‡‡æ ·å‚æ•°é…ç½®åŒº
        # ---------------------------------------------------------
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,       # æœ€å¤§ç”Ÿæˆé•¿åº¦
            
            do_sample=True,           # ã€å…³é”®ã€‘å¼€å¯é‡‡æ ·æ¨¡å¼ï¼Œå¦åˆ™ä¸‹é¢çš„ top_k/top_p ä¸ç”Ÿæ•ˆ
            
            temperature=0.1,          # æ¸©åº¦ï¼šè¶Šä½è¶Šä¿å®ˆ(0.1)ï¼Œè¶Šé«˜è¶Šå‘æ•£(1.0+)
            top_k=50,                 # Top-Kï¼šæ¯ä¸€æ­¥åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„ 50 ä¸ªè¯
            top_p=0.9,                # Top-P (æ ¸é‡‡æ ·)ï¼šåªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° 90% çš„è¯
            
            repetition_penalty=1.1,   # é‡å¤æƒ©ç½šï¼š>1.0 è¡¨ç¤ºæƒ©ç½šé‡å¤å†…å®¹ï¼Œå‡å°‘å¤è¯»æœºç°è±¡
            
            pad_token_id=tokenizer.eos_token_id # é˜²æ­¢è­¦å‘Š
        )
        # ---------------------------------------------------------
        
        # åªè§£ç æ–°ç”Ÿæˆçš„ token
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"\nğŸ¤– æ¨¡å‹å›å¤:\n{response}")
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    test_model()
